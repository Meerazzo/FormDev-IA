services:
  inference:
    image: vllm/vllm-openai:latest
    command: >
      --model ${MODEL_ID}
      --dtype ${DTYPE}
      --max-model-len ${MAX_MODEL_LEN}
      --host 0.0.0.0
      --port ${VLLM_PORT}
    volumes:
      - ${HF_CACHE}:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    restart: unless-stopped

  api:
    build:
      context: ..
      dockerfile: apps/api/Dockerfile
    environment:
      - VLLM_BASE_URL=http://inference:${VLLM_PORT}
      - API_KEYS=${API_KEYS}
      - RATE_LIMIT_RPM=${RATE_LIMIT_RPM}
    ports:
      - "${API_PORT}:8080"
    depends_on:
      - inference
    restart: unless-stopped